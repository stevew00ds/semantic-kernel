{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "68e1c158",
            "metadata": {},
            "source": [
                "# Using Hugging Face With Plugins\n",
                "\n",
                "In this notebook, we demonstrate using Hugging Face models for Plugins using both SemanticMemory and text completions.\n",
                "\n",
                "SK supports downloading models from the Hugging Face that can perform the following tasks: text-generation, text2text-generation, summarization, and sentence-similarity. You can search for models by task at https://huggingface.co/models.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a77bdf89",
            "metadata": {},
            "outputs": [],
            "source": [
                "!python -m pip install semantic-kernel[hugging_face]==0.9.6b1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f570d466",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip3 install torch torchvision torchaudio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "508ad44f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/steve/miniconda3/envs/azure/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "/home/steve/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
                        "  warnings.warn(\n",
                        "2024-04-29 14:33:18.237908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2024-04-29 14:33:18.282223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2024-04-29 14:33:19.127862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
                    ]
                }
            ],
            "source": [
                "import semantic_kernel as sk\n",
                "import semantic_kernel.connectors.ai.hugging_face as sk_hf\n",
                "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "753ab756",
            "metadata": {},
            "outputs": [],
            "source": [
                "from services import Service\n",
                "\n",
                "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
                "selectedService = Service.HuggingFace"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "d8ddffc1",
            "metadata": {},
            "source": [
                "First, we will create a kernel and add both text completion and embedding services.\n",
                "\n",
                "For text completion, we are choosing GPT2. This is a text-generation model. (Note: text-generation will repeat the input in the output, text2text-generation will not.)\n",
                "For embeddings, we are using sentence-transformers/all-MiniLM-L6-v2. Vectors generated for this model are of length 384 (compared to a length of 1536 from OpenAI ADA).\n",
                "\n",
                "The following step may take a few minutes when run for the first time as the models will be downloaded to your local machine.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "8f8dcbc6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantic_kernel import Kernel\n",
                "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion, HuggingFaceTextEmbedding\n",
                "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
                "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
                "\n",
                "kernel = Kernel()\n",
                "\n",
                "# Configure LLM service\n",
                "if selectedService == Service.HuggingFace:\n",
                "    # Feel free to update this model to any other model available on Hugging Face\n",
                "    text_service_id = \"HuggingFaceM4/tiny-random-LlamaForCausalLM\"\n",
                "    kernel.add_service(\n",
                "        service=HuggingFaceTextCompletion(\n",
                "            service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
                "        ),\n",
                "    )\n",
                "    embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "    embedding_svc = HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
                "    kernel.add_service(\n",
                "        service=embedding_svc,\n",
                "    )\n",
                "    memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
                "    kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2a7e7ca4",
            "metadata": {},
            "source": [
                "### Add Memories and Define a plugin to use them\n",
                "\n",
                "Most models available on huggingface.co are not as powerful as OpenAI GPT-3+. Your plugins will likely need to be simpler to accommodate this.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d096504c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantic_kernel.connectors.ai.hugging_face import HuggingFacePromptExecutionSettings\n",
                "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
                "\n",
                "collection_id = \"generic\"\n",
                "\n",
                "await memory.save_information(collection=collection_id, id=\"info1\", text=\"Sharks are fish.\")\n",
                "await memory.save_information(collection=collection_id, id=\"info2\", text=\"Whales are mammals.\")\n",
                "await memory.save_information(collection=collection_id, id=\"info3\", text=\"Penguins are birds.\")\n",
                "await memory.save_information(collection=collection_id, id=\"info4\", text=\"Dolphins are mammals.\")\n",
                "await memory.save_information(collection=collection_id, id=\"info5\", text=\"Flies are insects.\")\n",
                "\n",
                "# Define prompt function using SK prompt template language\n",
                "my_prompt = \"\"\"I know these animal facts: \n",
                "- {{recall 'fact about sharks'}}\n",
                "- {{recall 'fact about whales'}} \n",
                "- {{recall 'fact about penguins'}} \n",
                "- {{recall 'fact about dolphins'}} \n",
                "- {{recall 'fact about flies'}}\n",
                "Now, tell me something about: {{$request}}\"\"\"\n",
                "\n",
                "execution_settings = HuggingFacePromptExecutionSettings(\n",
                "    service_id=text_service_id,\n",
                "    ai_model_id=text_service_id,\n",
                "    max_tokens=45,\n",
                "    temperature=0.5,\n",
                "    top_p=0.5,\n",
                ")\n",
                "\n",
                "prompt_template_config = PromptTemplateConfig(\n",
                "    template=my_prompt,\n",
                "    name=\"text_complete\",\n",
                "    template_format=\"semantic-kernel\",\n",
                "    execution_settings=execution_settings,\n",
                ")\n",
                "\n",
                "my_function = kernel.add_function(\n",
                "    function_name=\"text_complete\",\n",
                "    plugin_name=\"TextCompletionPlugin\",\n",
                "    prompt_template_config=prompt_template_config,\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2calf857",
            "metadata": {},
            "source": [
                "Let's now see what the completion looks like! Remember, \"gpt2\" is nowhere near as large as ChatGPT, so expect a much simpler answer.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "628c843e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/steve/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
                        "  warnings.warn(\n",
                        "/home/steve/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
                        "  warnings.warn(\n",
                        "/home/steve/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
                        "  warnings.warn(\n",
                        "Error occurred while invoking function text_complete: Error occurred while invoking function text_complete: ('Hugging Face completion failed', ValueError('Input length of input_ids is 65, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.'))\n"
                    ]
                },
                {
                    "ename": "KernelInvokeException",
                    "evalue": "Error occurred while invoking function: 'TextCompletionPlugin-text_complete'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/connectors/ai/hugging_face/services/hf_text_completion.py:96\u001b[0m, in \u001b[0;36mHuggingFaceTextCompletion.complete\u001b[0;34m(self, prompt, settings)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_settings_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/pipelines/base.py:1242\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/pipelines/base.py:1249\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1248\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1249\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1149\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/generation/utils.py:1499\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/transformers/generation/utils.py:1149\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1153\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n",
                        "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 65, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/functions/kernel_function_from_prompt.py:213\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._handle_text_complete\u001b[0;34m(self, service, execution_settings, prompt, arguments)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m service\u001b[38;5;241m.\u001b[39mcomplete(prompt, execution_settings)\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_function_result(completions, \u001b[38;5;28;01mNone\u001b[39;00m, arguments, prompt\u001b[38;5;241m=\u001b[39mprompt)\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/connectors/ai/hugging_face/services/hf_text_completion.py:98\u001b[0m, in \u001b[0;36mHuggingFaceTextCompletion.complete\u001b[0;34m(self, prompt, settings)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHugging Face completion failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mlist\u001b[39m):\n",
                        "\u001b[0;31mServiceResponseException\u001b[0m: ('Hugging Face completion failed', ValueError('Input length of input_ids is 65, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.'))",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mFunctionExecutionException\u001b[0m                Traceback (most recent call last)",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/functions/kernel_function.py:190\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[0;34m(self, kernel, arguments, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_internal(kernel, arguments)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/functions/kernel_function_from_prompt.py:163\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[0;34m(self, kernel, arguments)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(service, TextCompletionClientBase):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_text_complete(\n\u001b[1;32m    164\u001b[0m         service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[1;32m    165\u001b[0m         execution_settings\u001b[38;5;241m=\u001b[39mexecution_settings,\n\u001b[1;32m    166\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    167\u001b[0m         arguments\u001b[38;5;241m=\u001b[39marguments,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(service)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not a valid AI service\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/functions/kernel_function_from_prompt.py:216\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._handle_text_complete\u001b[0;34m(self, service, execution_settings, prompt, arguments)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
                        "\u001b[0;31mFunctionExecutionException\u001b[0m: Error occurred while invoking function text_complete: ('Hugging Face completion failed', ValueError('Input length of input_ids is 65, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.'))",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kernel\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      2\u001b[0m     my_function,\n\u001b[1;32m      3\u001b[0m     request\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are whales?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(output)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      8\u001b[0m query_result1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m memory\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m      9\u001b[0m     collection\u001b[38;5;241m=\u001b[39mcollection_id, query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are sharks?\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_relevance_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
                        "File \u001b[0;32m~/miniconda3/envs/azure/lib/python3.10/site-packages/semantic_kernel/kernel.py:277\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[0;34m(self, function, arguments, function_name, plugin_name, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m function_invoked_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_function_invoked(function\u001b[38;5;241m.\u001b[39mmetadata, arguments, function_result, exception)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_invoked_args\u001b[38;5;241m.\u001b[39mexception:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunction_invoked_args\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_invoked_args\u001b[38;5;241m.\u001b[39mis_cancel_requested:\n\u001b[1;32m    281\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution was cancelled on function invoked event of function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m     )\n",
                        "\u001b[0;31mKernelInvokeException\u001b[0m: Error occurred while invoking function: 'TextCompletionPlugin-text_complete'"
                    ]
                }
            ],
            "source": [
                "output = await kernel.invoke(\n",
                "    my_function,\n",
                "    request=\"What are whales?\",\n",
                ")\n",
                "\n",
                "output = str(output).strip()\n",
                "\n",
                "query_result1 = await memory.search(\n",
                "    collection=collection_id, query=\"What are sharks?\", limit=1, min_relevance_score=0.3\n",
                ")\n",
                "\n",
                "print(f\"The queried result for 'What are sharks?' is {query_result1[0].text}\")\n",
                "\n",
                "print(f\"{text_service_id} completed prompt with: '{output}'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
